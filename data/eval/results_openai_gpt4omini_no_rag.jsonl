{"id": 1, "question": "What is the main objective of the EU AI Act?", "gold_answer": "The main objective of the EU AI Act is to ensure the proper functioning of the internal market by establishing a uniform legal framework for the development, commercialization, and use of AI systems in the European Union. It aims to promote trustworthy, human-centric AI while ensuring a high level of protection of health, safety, fundamental rights, and the environment.", "model_answer": "The main objective of the EU AI Act is to establish a comprehensive regulatory framework for artificial intelligence within the European Union. The Act aims to ensure that AI systems are safe, respect fundamental rights, and promote trust in AI technologies. It seeks to balance innovation and safety by categorizing AI systems based on their risk levels—ranging from minimal to unacceptable risk—and imposing corresponding requirements and obligations on developers and users. The overarching goal is to foster a trustworthy AI ecosystem that supports economic growth while safeguarding public interests and ethical standards.", "contexts": []}
{"id": 2, "question": "How does the AI Act define an AI system?", "gold_answer": "The AI Act defines an AI system as a machine-based system designed to operate with varying levels of autonomy that can, for explicit or implicit objectives, generate outputs such as predictions, recommendations, or decisions influencing physical or virtual environments.", "model_answer": "The EU AI Act defines an AI system as a software that is developed with one or more of the techniques and approaches listed in the regulation, including machine learning, logic- and knowledge-based approaches, and statistical approaches. The definition emphasizes that an AI system can perform tasks that would typically require human intelligence, such as understanding natural language, recognizing patterns, or making decisions.\n\nThe Act also specifies that AI systems can be used in various applications and sectors, and it categorizes them based on the level of risk they pose to fundamental rights and safety. This risk-based approach is central to the regulation, as it aims to ensure that high-risk AI systems are subject to stricter requirements and oversight.\n\nOverall, the definition is designed to encompass a broad range of AI technologies while focusing on their impact on society and individuals.", "contexts": []}
{"id": 3, "question": "To whom does the AI Act apply?", "gold_answer": "The AI Act applies to providers, deployers, importers, distributors, and manufacturers of AI systems placed on the EU market or whose outputs are used in the EU. It also applies to users outside the EU if their AI systems affect people located in the Union.", "model_answer": "The EU AI Act applies to a wide range of stakeholders involved in the development, deployment, and use of artificial intelligence systems within the European Union. Specifically, it targets:\n\n1. **Providers of AI systems**: This includes organizations or individuals who develop or place AI systems on the market, whether they are based in the EU or outside the EU, as long as their AI systems are intended for use within the EU.\n\n2. **Users of AI systems**: This refers to individuals or organizations that use AI systems in the EU, regardless of where the provider is located.\n\n3. **Importers and distributors**: Entities that import AI systems into the EU or distribute them within the EU are also subject to the regulations set forth in the AI Act.\n\n4. **AI systems**: The Act applies to a broad range of AI systems, including those that are high-risk, as well as those that may not fall into the high-risk category but still have implications for safety, fundamental rights, and ethical considerations.\n\nThe AI Act aims to ensure that AI systems used in the EU are safe, respect fundamental rights, and promote trust in AI technologies.", "contexts": []}
{"id": 4, "question": "Which AI systems are prohibited under the AI Act?", "gold_answer": "The AI Act prohibits AI systems that use subliminal techniques or exploit vulnerabilities to distort behavior, social scoring systems by public authorities, and remote biometric identification systems for law enforcement in real time in public spaces, except in specific narrowly defined cases.", "model_answer": "Under the EU AI Act, certain AI systems are classified as high-risk and are subject to strict regulations, while others are outright prohibited. The AI systems that are prohibited include:\n\n1. **Social scoring by governments**: This refers to systems that evaluate or classify individuals based on their social behavior or characteristics, which could lead to discrimination or social control.\n\n2. **Real-time biometric identification in public spaces**: The use of AI systems for real-time facial recognition in public areas is prohibited, except for specific cases such as law enforcement in urgent situations.\n\n3. **Manipulative AI systems**: This includes AI systems that exploit vulnerabilities of specific groups, such as children or individuals with disabilities, to manipulate their behavior.\n\n4. **Systems that create deep fakes**: AI technologies that generate misleading or harmful content, particularly in a way that could deceive individuals or the public, are also prohibited.\n\nThese prohibitions are aimed at protecting fundamental rights and ensuring ethical use of AI technologies within the EU. The regulations are part of a broader effort to create a safe and trustworthy AI environment.", "contexts": []}
{"id": 5, "question": "What is considered a high-risk AI system under the AI Act?", "gold_answer": "High-risk AI systems are those that pose significant risks to health, safety, or fundamental rights and are listed in Annex III of the Act, including systems used in biometric identification, critical infrastructure, education, employment, essential services, law enforcement, migration control, and justice.", "model_answer": "Under the EU AI Act, a high-risk AI system is defined based on its potential impact on fundamental rights and safety. Specifically, high-risk AI systems are those that:\n\n1. **Are used in critical areas**: This includes sectors such as healthcare, transportation, law enforcement, and education, where the use of AI can significantly affect individuals' rights and safety.\n\n2. **Meet specific criteria**: The Act outlines certain criteria that determine whether an AI system is high-risk, including:\n   - The intended purpose of the AI system.\n   - The potential for significant harm to individuals or society.\n   - The context in which the AI is deployed.\n\n3. **Fall under specific categories**: The Act provides a list of specific applications that are considered high-risk, such as:\n   - Biometric identification and categorization of natural persons.\n   - Critical infrastructure management (e.g., energy, transport).\n   - Education and vocational training (e.g., scoring exams).\n   - Employment and worker management (e.g., recruitment tools).\n   - Access to essential services (e.g., credit scoring).\n\nHigh-risk AI systems are subject to stricter regulatory requirements, including risk management, data governance, transparency, and human oversight, to ensure they operate safely and ethically.", "contexts": []}
{"id": 6, "question": "What obligations must providers of high-risk AI systems fulfill?", "gold_answer": "Providers of high-risk AI systems must implement risk management, ensure data governance quality, maintain technical documentation, establish logging capabilities, provide transparency to users, enable human oversight, and guarantee robustness, accuracy, and cybersecurity before placing the system on the market.", "model_answer": "Under the EU AI Act, providers of high-risk AI systems are subject to several obligations aimed at ensuring the safety and accountability of these technologies. Key obligations include:\n\n1. **Risk Management System**: Providers must establish and maintain a risk management system to identify, assess, and mitigate risks associated with their AI systems throughout the lifecycle.\n\n2. **Data Governance and Management**: High-risk AI systems must be trained on high-quality datasets that are relevant, representative, and free from biases. Providers must ensure that data used is compliant with data protection regulations.\n\n3. **Technical Documentation**: Providers are required to create and maintain comprehensive technical documentation that demonstrates compliance with the regulation, including details about the system's design, development, and intended use.\n\n4. **Transparency and Information Provision**: Providers must ensure that users are informed about the AI system's capabilities and limitations. This includes providing clear information on how the system operates and the data it uses.\n\n5. **Human Oversight**: High-risk AI systems must be designed to allow for human oversight. This means that users should be able to intervene or override the system's decisions when necessary.\n\n6. **Conformity Assessment**: Before placing a high-risk AI system on the market, providers must undergo a conformity assessment to ensure compliance with the relevant requirements. This may involve third-party evaluation.\n\n7. **Post-Market Monitoring**: Providers are obligated to monitor the performance of their AI systems after deployment and take corrective actions if any issues arise.\n\n8. **Reporting Obligations**: Providers must report any serious incidents or malfunctions of their AI systems to the relevant authorities.\n\n9. **Registration**: High-risk AI systems may need to be registered in a European database, depending on the specific requirements set forth in the regulation.\n\nThese obligations are designed to ensure that high-risk AI systems are safe, reliable, and respect fundamental rights, thereby fostering trust in AI technologies across the EU.", "contexts": []}
{"id": 7, "question": "What obligations do deployers of high-risk AI systems have?", "gold_answer": "Deployers of high-risk AI systems must use the systems according to instructions, ensure human oversight, monitor performance, maintain logs, and report serious incidents to providers or authorities.", "model_answer": "Under the EU AI Act, deployers of high-risk AI systems have several key obligations aimed at ensuring the safe and ethical use of AI technologies. These obligations include:\n\n1. **Risk Management System**: Deployers must establish a risk management system to identify, assess, and mitigate risks associated with the AI system throughout its lifecycle.\n\n2. **Data Governance and Management**: High-risk AI systems must be trained and tested on high-quality datasets that are representative, free from biases, and relevant to the intended purpose of the system. Deployers are responsible for ensuring data quality and compliance with data protection regulations.\n\n3. **Technical Documentation**: Deployers must maintain comprehensive technical documentation that details the design, development, and functioning of the AI system. This documentation should facilitate compliance checks and audits.\n\n4. **Transparency and Information Provision**: Deployers are required to provide clear and adequate information about the AI system's capabilities, limitations, and intended use. This includes informing users about the system's functioning and any potential risks.\n\n5. **Human Oversight**: High-risk AI systems must be designed to allow for human oversight. Deployers must ensure that users can intervene or override the system when necessary, particularly in critical applications.\n\n6. **Post-Market Monitoring**: After deployment, there is an obligation for ongoing monitoring of the AI system's performance and impact. Deployers must have mechanisms in place to identify and address any issues that arise during the system's operation.\n\n7. **Compliance with Conformity Assessment**: Before placing a high-risk AI system on the market, deployers must undergo a conformity assessment to demonstrate compliance with the relevant requirements of the AI Act.\n\n8. **Reporting Obligations**: Deployers must report any serious incidents or malfunctions related to the AI system to the relevant authorities, as well as take corrective actions as necessary.\n\nThese obligations are designed to ensure that high-risk AI systems are developed and deployed in a manner that prioritizes safety, accountability, and respect for fundamental rights.", "contexts": []}
{"id": 8, "question": "What are the requirements for training data of high-risk AI systems?", "gold_answer": "Training, validation, and testing data for high-risk AI systems must be relevant, representative, free of errors, and sufficiently complete to avoid bias, ensuring safety and respect of fundamental rights.", "model_answer": "Under the EU AI Act, high-risk AI systems are subject to stringent requirements regarding their training data to ensure safety, fairness, and transparency. The key requirements for training data of high-risk AI systems include:\n\n1. **Quality and Representativeness**: The training data must be of high quality, representative of the intended use of the AI system, and cover a wide range of scenarios to minimize bias and ensure that the AI system performs reliably across different contexts.\n\n2. **Data Governance**: Organizations must implement robust data governance practices to ensure the integrity and accuracy of the training data. This includes maintaining documentation of the data sources and methodologies used in data collection.\n\n3. **Bias Mitigation**: There should be measures in place to identify and mitigate any potential biases in the training data. This is crucial to prevent discrimination and ensure that the AI system does not produce unfair outcomes.\n\n4. **Traceability**: The data used for training must be traceable, meaning that organizations should be able to document the origin of the data and how it was processed. This is important for accountability and compliance with regulatory requirements.\n\n5. **Compliance with Data Protection Laws**: The training data must comply with relevant data protection regulations, such as the General Data Protection Regulation (GDPR). This includes ensuring that personal data is handled appropriately and that individuals' rights are respected.\n\n6. **Documentation and Transparency**: Organizations are required to maintain detailed documentation regarding the training data, including its characteristics, the rationale for its selection, and any preprocessing steps taken. This documentation should be made available to relevant authorities upon request.\n\nThese requirements aim to ensure that high-risk AI systems are developed and deployed responsibly, minimizing risks to individuals and society while promoting trust in AI technologies.", "contexts": []}
{"id": 9, "question": "How does the AI Act regulate general-purpose AI (GPAI) models?", "gold_answer": "Providers of general-purpose AI models must comply with transparency obligations, including documentation, dataset descriptions, copyright compliance, and information enabling responsible use. For systemic-risk GPAI models, stricter risk-mitigation and cybersecurity requirements apply.", "model_answer": "The EU AI Act, which is part of the European Commission's broader regulatory framework for artificial intelligence, addresses general-purpose AI (GPAI) models by categorizing them based on their risk levels and establishing specific obligations for their deployment and use.\n\n1. **Risk-Based Classification**: The AI Act classifies AI systems into different risk categories: unacceptable risk, high risk, limited risk, and minimal risk. GPAI models, depending on their intended use and the context in which they are deployed, may fall into the high-risk category if they are used in critical applications (e.g., healthcare, transportation).\n\n2. **Compliance Requirements**: For GPAI models classified as high-risk, the Act imposes stringent requirements, including:\n   - **Risk Management**: Developers must implement risk management systems to identify and mitigate potential risks associated with the AI model.\n   - **Data Governance**: There are requirements for data quality, including ensuring that training data is representative and free from bias.\n   - **Transparency**: Users must be informed about the capabilities and limitations of the AI system, including how decisions are made.\n   - **Human Oversight**: There must be provisions for human oversight to ensure that AI systems do not operate autonomously without appropriate human intervention.\n\n3. **Documentation and Reporting**: Developers of GPAI models are required to maintain detailed documentation of the AI system's design, development, and testing processes. This documentation must be made available for regulatory scrutiny.\n\n4. **Post-Market Monitoring**: There are obligations for ongoing monitoring of the AI system's performance and impact after it has been deployed, ensuring that any issues can be addressed promptly.\n\n5. **Accountability**: The Act emphasizes accountability, requiring that entities deploying GPAI models can be held responsible for any harm caused by their use.\n\nOverall, the regulation aims to ensure that GPAI models are developed and used in a manner that is safe, ethical, and respects fundamental rights, while also fostering innovation within the AI sector.", "contexts": []}
{"id": 10, "question": "What documentation must GPAI model providers make available?", "gold_answer": "Providers of GPAI models must publish a detailed description of training data, system capabilities and limitations, evaluation results, model architecture overview, and recommended uses and restrictions.", "model_answer": "Under the EU AI Act, model providers, including those involved in General Purpose AI (GPAI), are required to ensure transparency and accountability in their operations. While the specific documentation requirements can vary based on the classification of the AI system (e.g., high-risk vs. low-risk), generally, the following documentation must be made available:\n\n1. **Technical Documentation**: This includes detailed information about the design, development, and functioning of the AI model. It should cover aspects such as the architecture, algorithms used, training data, and performance metrics.\n\n2. **Risk Assessment**: Providers must conduct and document a risk assessment that identifies potential risks associated with the AI model, including ethical, legal, and societal implications.\n\n3. **Data Management Practices**: Documentation regarding the data used for training the model, including data sources, data quality, and data governance practices, must be provided.\n\n4. **User Instructions**: Clear guidelines and instructions for users on how to properly use the AI system, including any limitations and potential risks associated with its use.\n\n5. **Compliance Evidence**: Documentation demonstrating compliance with relevant EU regulations and standards, including any assessments or certifications obtained.\n\n6. **Monitoring and Maintenance Procedures**: Information on how the model will be monitored post-deployment, including procedures for updates, maintenance, and handling of any identified issues.\n\n7. **Transparency Reports**: Depending on the model's risk classification, there may be requirements for periodic reports that provide insights into the model's performance and any incidents or issues encountered.\n\nThese documentation requirements aim to promote transparency, accountability, and trust in AI systems, especially those that are widely used or have significant societal impacts.", "contexts": []}
{"id": 11, "question": "Does the AI Act apply to AI systems used for purely personal activities?", "gold_answer": "No. The AI Act does not apply to natural persons using AI systems strictly for personal, non-professional activities.", "model_answer": "The EU AI Act primarily targets AI systems that have a significant impact on individuals and society, particularly in areas such as safety, fundamental rights, and the functioning of the market. Generally, the Act focuses on AI systems that are used in professional, commercial, or public contexts rather than those used solely for personal activities.\n\nAs a result, AI systems that are employed exclusively for personal use—such as personal assistants, hobby projects, or other non-commercial applications—are typically not covered by the AI Act. However, there may be exceptions if the AI system poses risks to fundamental rights or safety, even in personal contexts. \n\nIn summary, while the AI Act does not generally apply to AI systems used for purely personal activities, specific circumstances could lead to certain provisions being relevant.", "contexts": []}
{"id": 12, "question": "What obligations apply to importers of AI systems?", "gold_answer": "Importers must ensure that AI systems from third countries comply with the AI Act, verify CE marking, ensure technical documentation availability, and refrain from placing non-compliant systems on the EU market.", "model_answer": "Under the EU AI Act, importers of AI systems have specific obligations to ensure compliance with the regulation. These obligations include:\n\n1. **Conformity Assessment**: Importers must ensure that the AI systems they import have undergone the necessary conformity assessment procedures. This involves verifying that the systems meet the requirements set out in the regulation, particularly for high-risk AI systems.\n\n2. **Documentation**: Importers are required to maintain and provide access to relevant documentation that demonstrates compliance with the EU AI Act. This may include technical documentation, risk assessments, and evidence of conformity assessments.\n\n3. **Labeling and Traceability**: Importers must ensure that AI systems are appropriately labeled and that they can be traced back to their manufacturers. This includes providing information about the origin of the AI system and any relevant certifications.\n\n4. **Reporting Obligations**: Importers have a duty to report any incidents or non-compliance issues related to the AI systems they import. This includes notifying authorities if they become aware that an AI system poses a risk to health, safety, or fundamental rights.\n\n5. **Cooperation with Authorities**: Importers must cooperate with national authorities in the event of inspections or investigations related to the AI systems they import. This includes providing access to necessary documentation and information.\n\n6. **Market Surveillance**: Importers are responsible for monitoring the AI systems they place on the market to ensure ongoing compliance with the EU AI Act and to take corrective actions if any issues arise.\n\nThese obligations aim to enhance the safety and accountability of AI systems within the EU market, ensuring that imported systems meet the same standards as those developed within the EU.", "contexts": []}
{"id": 13, "question": "What is the conformity assessment for high-risk AI systems?", "gold_answer": "Conformity assessment verifies that a high-risk AI system meets all regulatory requirements before being placed on the market, including risk management, data quality, documentation, human oversight, and cybersecurity.", "model_answer": "Under the EU AI Act, high-risk AI systems are subject to a conformity assessment process to ensure they comply with the regulatory requirements set forth in the legislation. The conformity assessment aims to evaluate whether these systems meet the necessary safety, transparency, and accountability standards before they can be placed on the market or put into service.\n\nThe conformity assessment process typically involves the following steps:\n\n1. **Risk Classification**: AI systems are classified based on their risk level. High-risk AI systems are those that pose significant risks to health, safety, or fundamental rights.\n\n2. **Documentation and Technical Files**: Developers must prepare comprehensive documentation that includes technical files demonstrating compliance with the requirements of the AI Act. This documentation should cover aspects such as design, development, testing, and risk management.\n\n3. **Assessment by Notified Bodies**: For high-risk AI systems, the conformity assessment must often be conducted by an independent third-party organization known as a \"notified body.\" These bodies are designated by EU member states and are responsible for evaluating the compliance of AI systems with the relevant standards.\n\n4. **Evaluation of Compliance**: The notified body assesses the technical documentation and may conduct audits, inspections, or testing to verify that the AI system meets the required standards.\n\n5. **Issuance of Conformity Assessment Certificate**: If the AI system is found to be compliant, the notified body issues a conformity assessment certificate, which allows the developer to affix the CE marking to the product, indicating compliance with EU regulations.\n\n6. **Post-Market Monitoring**: After the AI system is deployed, there are ongoing obligations for monitoring its performance and reporting any issues that arise, ensuring continued compliance with the regulations.\n\nOverall, the conformity assessment process for high-risk AI systems is designed to enhance safety and trust in AI technologies, ensuring that they operate within the legal framework established by the EU.", "contexts": []}
{"id": 14, "question": "What are post-market monitoring obligations for providers?", "gold_answer": "Providers must maintain a post-market monitoring system, collect and analyze real-world performance data, identify malfunctions or risks, and take corrective actions when needed.", "model_answer": "Under the EU AI Act, post-market monitoring obligations for providers are designed to ensure that AI systems continue to operate safely and effectively after they have been placed on the market. These obligations typically include the following key elements:\n\n1. **Continuous Monitoring**: Providers must continuously monitor the performance of their AI systems in real-world conditions to identify any potential risks or issues that may arise after deployment.\n\n2. **Data Collection**: Providers are required to collect relevant data on the operation of their AI systems, including user feedback, performance metrics, and any incidents or malfunctions that occur.\n\n3. **Risk Management**: Providers must implement a risk management system that allows them to assess and mitigate risks associated with their AI systems throughout their lifecycle.\n\n4. **Reporting Obligations**: Providers may be required to report any significant incidents or safety concerns to the relevant authorities and, in some cases, to users or affected parties.\n\n5. **Updates and Improvements**: Based on the findings from post-market monitoring, providers should be prepared to update their AI systems to address any identified issues or to improve performance.\n\n6. **Documentation**: Providers must maintain comprehensive documentation of their post-market monitoring activities, including the methodologies used, findings, and any actions taken in response to identified risks.\n\n7. **Collaboration with Authorities**: Providers may need to cooperate with regulatory authorities during inspections or audits related to post-market monitoring.\n\nThese obligations aim to ensure that AI systems remain compliant with safety and ethical standards, thereby protecting users and society at large from potential harms associated with AI technologies.", "contexts": []}
{"id": 15, "question": "When must a serious incident involving a high-risk AI system be reported?", "gold_answer": "A serious incident must be reported by deployers or providers to competent authorities no later than 15 days after becoming aware of it.", "model_answer": "Under the EU AI Act, a serious incident involving a high-risk AI system must be reported without undue delay, and in any case within 15 days of the incident occurring. This requirement is part of the regulatory framework aimed at ensuring accountability and transparency in the deployment of high-risk AI systems. The reporting should include details about the nature of the incident, its impact, and any measures taken in response. This regulation is designed to enhance safety and trust in AI technologies across the EU.", "contexts": []}
